{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# forensicface--A tool for forensic face examination\n",
    "\n",
    "> An integrated tool to compare faces using state-of-the-art face recognition models and compute Likelihood Ratios "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp app\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.utils import *\n",
    "import onnxruntime\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "from glob import glob\n",
    "from insightface.app import FaceAnalysis\n",
    "from insightface.utils import face_align\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class ForensicFace:\n",
    "    \"A (forensic) face comparison tool\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str = \"sepaelv2\",\n",
    "        det_size: int = 320,\n",
    "        use_gpu: bool = True,\n",
    "        gpu: int = 0,  # which GPU to use\n",
    "        magface=False,\n",
    "        extended=True,\n",
    "    ):\n",
    "        self.extended = extended\n",
    "        if self.extended == True:\n",
    "            allowed_modules = [\"detection\", \"landmark_3d_68\", \"genderage\"]\n",
    "        else:\n",
    "            allowed_modules = [\"detection\"]\n",
    "\n",
    "        self.det_size = (det_size, det_size)\n",
    "\n",
    "        self.magface = magface\n",
    "\n",
    "        self.detectmodel = FaceAnalysis(\n",
    "            name=model,\n",
    "            allowed_modules=allowed_modules,\n",
    "            providers=[(\"CUDAExecutionProvider\", {\"device_id\": gpu})]\n",
    "            if use_gpu\n",
    "            else [\"CPUExecutionProvider\"],\n",
    "        )\n",
    "        self.detectmodel.prepare(ctx_id=gpu if use_gpu else -1, det_size=self.det_size)\n",
    "\n",
    "        onnx_rec_model = glob(\n",
    "            osp.join(\n",
    "                osp.expanduser(\"~/.insightface/models\"),\n",
    "                model,\n",
    "                \"adaface\",\n",
    "                \"adaface_*.onnx\",\n",
    "            )\n",
    "        )\n",
    "        assert len(onnx_rec_model) == 1\n",
    "        self.ort_ada = onnxruntime.InferenceSession(\n",
    "            onnx_rec_model[0],\n",
    "            providers=[(\"CUDAExecutionProvider\", {\"device_id\": gpu})]\n",
    "            if use_gpu\n",
    "            else [\"CPUExecutionProvider\"],\n",
    "        )\n",
    "\n",
    "        if self.magface:\n",
    "            self.ort_mag = onnxruntime.InferenceSession(\n",
    "                osp.join(\n",
    "                    osp.expanduser(\"~/.insightface/models\"),\n",
    "                    model,\n",
    "                    \"magface\",\n",
    "                    \"magface_iresnet100.onnx\",\n",
    "                ),\n",
    "                providers=[(\"CUDAExecutionProvider\", {\"device_id\": gpu})]\n",
    "                if use_gpu\n",
    "                else [\"CPUExecutionProvider\"],\n",
    "            )\n",
    "\n",
    "    def _to_input_ada(self, aligned_bgr_img):\n",
    "        _aligned_bgr_img = aligned_bgr_img.astype(np.float32)\n",
    "        _aligned_bgr_img = ((_aligned_bgr_img / 255.0) - 0.5) / 0.5\n",
    "        return _aligned_bgr_img.transpose(2, 0, 1).reshape(1, 3, 112, 112)\n",
    "\n",
    "    def _to_input_mag(self, aligned_bgr_img):\n",
    "        _aligned_bgr_img = aligned_bgr_img.astype(np.float32)\n",
    "        _aligned_bgr_img = _aligned_bgr_img / 255.0\n",
    "        return _aligned_bgr_img.transpose(2, 0, 1).reshape(1, 3, 112, 112)\n",
    "\n",
    "    def get_most_central_face(self, img, faces):\n",
    "        \"\"\"\n",
    "        faces is a insightface object with keypoints and bounding_box\n",
    "\n",
    "        return: keypoints of the most central face\n",
    "        \"\"\"\n",
    "        assert faces is not None\n",
    "        img_center = np.array([img.shape[0] // 2, img.shape[1] // 2])\n",
    "        dist = []\n",
    "\n",
    "        # Compute centers of faces and distances from certer of image\n",
    "        for idx, face in enumerate(faces):\n",
    "            box = face.bbox.astype(\"int\").flatten()\n",
    "            face_center = np.array([(box[0] + box[2]) // 2, (box[1] + box[3]) // 2])\n",
    "            dist.append(np.linalg.norm(img_center - face_center))\n",
    "\n",
    "        # Get index of the face closest to the center of image\n",
    "        idx = dist.index(min(dist))\n",
    "        return idx, faces[idx].kps\n",
    "\n",
    "    def get_larger_face(self, img, faces):\n",
    "        \"\"\"\n",
    "        faces is a insightface object with keypoints and bounding_box\n",
    "\n",
    "        return: keypoints of the larger face\n",
    "        \"\"\"\n",
    "        assert faces is not None\n",
    "        areas = []\n",
    "\n",
    "        # Compute centers of faces and distances from certer of image\n",
    "        for idx, face in enumerate(faces):\n",
    "            box = face.bbox.astype(\"int\").flatten()\n",
    "            areas.append(abs((box[2] - box[0]) * (box[3] - box[1])))\n",
    "\n",
    "        # Get index of the face closest to the center of image\n",
    "        idx = areas.index(max(areas))\n",
    "        return idx, faces[idx].kps\n",
    "\n",
    "    def process_image_single_face(self, imgpath: str):  # Path to image to be processed\n",
    "        \"\"\"\n",
    "        Process image and returns dict with:\n",
    "\n",
    "        - keypoints: 5 facial points (left eye, right eye, nose tip, left mouth corner and right mouth corner)\n",
    "\n",
    "        - ipd: interpupillary distance\n",
    "\n",
    "        - pitch, yaw, roll angles\n",
    "\n",
    "        - normalized_embedding\n",
    "\n",
    "        - embedding_norm\n",
    "\n",
    "        - aligned_face: face after alignment using the keypoints as references for affine transform\n",
    "\n",
    "        - (optional) magface norm and magface features\n",
    "        \"\"\"\n",
    "        if type(imgpath) == str:  # image path passed as argument\n",
    "            bgr_img = cv2.imread(imgpath)\n",
    "        else:  # image array passed as argument\n",
    "            bgr_img = imgpath.copy()\n",
    "        faces = self.detectmodel.get(bgr_img)\n",
    "        if len(faces) == 0:\n",
    "            return {}\n",
    "\n",
    "        idx, kps = self.get_larger_face(bgr_img, faces)\n",
    "\n",
    "        bbox = faces[idx].bbox.astype(\"int\")\n",
    "        bgr_aligned_face = face_align.norm_crop(bgr_img, kps)\n",
    "        ipd = np.linalg.norm(kps[0] - kps[1])\n",
    "\n",
    "        ada_inputs = {\n",
    "            self.ort_ada.get_inputs()[0].name: self._to_input_ada(bgr_aligned_face)\n",
    "        }\n",
    "        normalized_embedding, norm = self.ort_ada.run(None, ada_inputs)\n",
    "\n",
    "        ret = {\n",
    "            \"keypoints\": kps,\n",
    "            \"ipd\": ipd,\n",
    "            \"embedding\": normalized_embedding.flatten() * norm.flatten()[0],\n",
    "            \"norm\": norm.flatten()[0],\n",
    "            \"bbox\": bbox,\n",
    "            \"aligned_face\": cv2.cvtColor(bgr_aligned_face, cv2.COLOR_BGR2RGB),\n",
    "        }\n",
    "\n",
    "        if self.extended:\n",
    "            gender = \"M\" if faces[idx].gender == 1 else \"F\"\n",
    "            age = faces[idx].age\n",
    "            pitch, yaw, roll = faces[idx].pose\n",
    "            ret = {\n",
    "                **ret,\n",
    "                **{\n",
    "                    \"gender\": gender,\n",
    "                    \"age\": age,\n",
    "                    \"pitch\": pitch,\n",
    "                    \"yaw\": yaw,\n",
    "                    \"roll\": roll,\n",
    "                },\n",
    "            }\n",
    "\n",
    "        if self.magface:\n",
    "            # mag_inputs = {self.ort_mag.get_inputs()[0].name: self._to_input_mag(bgr_aligned_face)}\n",
    "            mag_embedding = self.ort_mag.run(None, ada_inputs)[0][0]\n",
    "            mag_norm = np.linalg.norm(mag_embedding)\n",
    "            ret = {\n",
    "                **ret,\n",
    "                **{\n",
    "                    \"magface_embedding\": mag_embedding,\n",
    "                    \"magface_norm\": mag_norm,\n",
    "                },\n",
    "            }\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def process_image(self, imgpath):\n",
    "        return self.process_image_single_face(imgpath)\n",
    "\n",
    "    def process_image_multiple_faces(\n",
    "        self,\n",
    "        imgpath: str,  # Path to image to be processed\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Process image and returns list of dicts with:\n",
    "\n",
    "        - keypoints: 5 facial points (left eye, right eye, nose tip, left mouth corner and right mouth corner)\n",
    "\n",
    "        - ipd: interpupillary distance\n",
    "\n",
    "        - pitch, yaw, roll angles\n",
    "\n",
    "        - normalized_embedding\n",
    "\n",
    "        - embedding_norm\n",
    "\n",
    "        - aligned_face: face after alignment using the keypoints as references for affine transform\n",
    "\n",
    "        - (optional) magface norm and magface features\n",
    "        \"\"\"\n",
    "        if type(imgpath) == str:  # image path passed as argument\n",
    "            bgr_img = cv2.imread(imgpath)\n",
    "        else:  # image array passed as argument\n",
    "            bgr_img = imgpath.copy()\n",
    "        faces = self.detectmodel.get(bgr_img)\n",
    "        if len(faces) == 0:\n",
    "            return []\n",
    "        ret = []\n",
    "        for face in faces:\n",
    "            kps = face.kps\n",
    "            bbox = face.bbox.astype(\"int\")\n",
    "            bgr_aligned_face = face_align.norm_crop(bgr_img, kps)\n",
    "            ipd = np.linalg.norm(kps[0] - kps[1])\n",
    "            ada_inputs = {\n",
    "                self.ort_ada.get_inputs()[0].name: self._to_input_ada(bgr_aligned_face)\n",
    "            }\n",
    "            normalized_embedding, norm = self.ort_ada.run(None, ada_inputs)\n",
    "            face_ret = {\n",
    "                \"keypoints\": kps,\n",
    "                \"ipd\": ipd,\n",
    "                \"embedding\": normalized_embedding.flatten() * norm.flatten()[0],\n",
    "                \"norm\": norm.flatten()[0],\n",
    "                \"bbox\": bbox,\n",
    "                \"aligned_face\": cv2.cvtColor(bgr_aligned_face, cv2.COLOR_BGR2RGB),\n",
    "            }\n",
    "\n",
    "            if self.extended:\n",
    "                gender = \"M\" if face.gender == 1 else \"F\"\n",
    "                age = face.age\n",
    "                pitch, yaw, roll = face.pose\n",
    "                face_ret = {\n",
    "                    **face_ret,\n",
    "                    **{\n",
    "                        \"gender\": gender,\n",
    "                        \"age\": age,\n",
    "                        \"pitch\": pitch,\n",
    "                        \"yaw\": yaw,\n",
    "                        \"roll\": roll,\n",
    "                    },\n",
    "                }\n",
    "\n",
    "            if self.magface:\n",
    "                # mag_inputs = {self.ort_mag.get_inputs()[0].name: self._to_input_mag(bgr_aligned_face)}\n",
    "                mag_embedding = self.ort_mag.run(None, ada_inputs)[0][0]\n",
    "                mag_norm = np.linalg.norm(mag_embedding)\n",
    "                face_ret = {\n",
    "                    **face_ret,\n",
    "                    **{\"magface_embedding\": mag_embedding, \"magface_norm\": mag_norm},\n",
    "                }\n",
    "\n",
    "            ret.append(face_ret)\n",
    "        return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'do_copy_in_default_stream': '1', 'arena_extend_strategy': 'kNextPowerOfTwo', 'gpu_external_empty_cache': '0', 'gpu_external_free': '0', 'cudnn_conv_use_max_workspace': '0', 'gpu_mem_limit': '18446744073709551615', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'gpu_external_alloc': '0', 'device_id': '0'}}\n",
      "find model: /home/rafael/.insightface/models/sepaelv2/1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'do_copy_in_default_stream': '1', 'arena_extend_strategy': 'kNextPowerOfTwo', 'gpu_external_empty_cache': '0', 'gpu_external_free': '0', 'cudnn_conv_use_max_workspace': '0', 'gpu_mem_limit': '18446744073709551615', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'gpu_external_alloc': '0', 'device_id': '0'}}\n",
      "find model: /home/rafael/.insightface/models/sepaelv2/det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'do_copy_in_default_stream': '1', 'arena_extend_strategy': 'kNextPowerOfTwo', 'gpu_external_empty_cache': '0', 'gpu_external_free': '0', 'cudnn_conv_use_max_workspace': '0', 'gpu_mem_limit': '18446744073709551615', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'gpu_external_alloc': '0', 'device_id': '0'}}\n",
      "find model: /home/rafael/.insightface/models/sepaelv2/genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "set det-size: (320, 320)\n"
     ]
    }
   ],
   "source": [
    "ff = ForensicFace(use_gpu=True, magface=False, extended=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rafael/mambaforge/envs/onnxgpu/lib/python3.9/site-packages/insightface/utils/transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(dict_keys(['keypoints', 'ipd', 'embedding', 'norm', 'bbox', 'aligned_face', 'gender', 'age', 'pitch', 'yaw', 'roll']),\n",
       " array([[471.42743, 418.60498],\n",
       "        [522.68933, 418.05362],\n",
       "        [498.82196, 449.08923],\n",
       "        [479.3499 , 476.44193],\n",
       "        [514.33453, 476.06885]], dtype=float32),\n",
       " array([441, 355, 548, 506]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = ff.process_image_multiple_faces(\"tela.png\")\n",
    "results[0].keys(), results[0][\"keypoints\"], results[0][\"bbox\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['keypoints', 'ipd', 'embedding', 'norm', 'bbox', 'aligned_face', 'gender', 'age', 'pitch', 'yaw', 'roll']),\n",
       " array([[103.60011, 139.88237],\n",
       "        [174.2651 , 137.3372 ],\n",
       "        [140.28094, 187.14757],\n",
       "        [109.09432, 219.3402 ],\n",
       "        [173.40782, 217.09576]], dtype=float32))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = ff.process_image_single_face(\"obama.png\")\n",
    "results.keys(), results[\"keypoints\"]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparação entre duas imagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "@patch\n",
    "def compare(self: ForensicFace, img1path: str, img2path: str):\n",
    "    img1data = self.process_image(img1path)\n",
    "    assert len(img1data) > 0\n",
    "    img2data = self.process_image(img2path)\n",
    "    assert len(img2data) > 0\n",
    "    return np.dot(img1data[\"embedding\"], img2data[\"embedding\"]) / (\n",
    "        img1data[\"norm\"] * img2data[\"norm\"]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8555971"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff.compare(\"obama.png\", \"obama2.png\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agregação de embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "@patch\n",
    "def aggregate_embeddings(self: ForensicFace, embeddings, weights=None):\n",
    "    if weights is None:\n",
    "        weights = np.ones(embeddings.shape[0], dtype=\"int\")\n",
    "    assert embeddings.shape[0] == weights.shape[0]\n",
    "    return np.average(embeddings, axis=0, weights=weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "@patch\n",
    "def aggregate_from_images(self: ForensicFace, list_of_image_paths):\n",
    "    embeddings = []\n",
    "    weights = []\n",
    "    for imgpath in list_of_image_paths:\n",
    "        d = self.process_image(imgpath)\n",
    "        if len(d) > 0:\n",
    "            embeddings.append(d[\"embedding\"])\n",
    "    if len(embeddings) > 0:\n",
    "        return self.aggregate_embeddings(np.array(embeddings))\n",
    "    else:\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512,)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated = ff.aggregate_from_images([\"obama.png\", \"obama2.png\"])\n",
    "aggregated.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suporte a MagFace\n",
    "\n",
    "Para utilizar, instancie o forensicface com a opção magface = True:\n",
    "\n",
    "``ff = forensicface(magface=True)``\n",
    "\n",
    "Modelo de [MagFace](https://github.com/IrvingMeng/MagFace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'do_copy_in_default_stream': '1', 'arena_extend_strategy': 'kNextPowerOfTwo', 'gpu_external_empty_cache': '0', 'gpu_external_free': '0', 'cudnn_conv_use_max_workspace': '0', 'gpu_mem_limit': '18446744073709551615', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'gpu_external_alloc': '0', 'device_id': '0'}}\n",
      "find model: /home/rafael/.insightface/models/sepaelv2/1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'do_copy_in_default_stream': '1', 'arena_extend_strategy': 'kNextPowerOfTwo', 'gpu_external_empty_cache': '0', 'gpu_external_free': '0', 'cudnn_conv_use_max_workspace': '0', 'gpu_mem_limit': '18446744073709551615', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'gpu_external_alloc': '0', 'device_id': '0'}}\n",
      "find model: /home/rafael/.insightface/models/sepaelv2/det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'do_copy_in_default_stream': '1', 'arena_extend_strategy': 'kNextPowerOfTwo', 'gpu_external_empty_cache': '0', 'gpu_external_free': '0', 'cudnn_conv_use_max_workspace': '0', 'gpu_mem_limit': '18446744073709551615', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'gpu_external_alloc': '0', 'device_id': '0'}}\n",
      "find model: /home/rafael/.insightface/models/sepaelv2/genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "set det-size: (320, 320)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(23.233418, 22.57745)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff = ForensicFace(det_size=320, use_gpu=True, magface=True)\n",
    "good = ff.process_image(\"001_frontal.JPG\")\n",
    "bad = ff.process_image(\"001_cam1_1.jpg\")\n",
    "good[\"magface_norm\"], bad[\"magface_norm\"]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extração de faces de vídeos com margem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch\n",
    "def _get_extended_bbox(self: ForensicFace, bbox, frame_shape, margin_factor):\n",
    "    # add a margin on the bounding box\n",
    "    (startX, startY, endX, endY) = bbox.astype(\"int\")\n",
    "    (h, w) = frame_shape[:2]\n",
    "    out_width = (endX - startX) * margin_factor\n",
    "    out_height = (endY - startY) * margin_factor\n",
    "\n",
    "    startX_out = int((startX + endX) / 2 - out_width / 2)\n",
    "    endX_out = int((startX + endX) / 2 + out_width / 2)\n",
    "    startY_out = int((startY + endY) / 2 - out_height / 2)\n",
    "    endY_out = int((startY + endY) / 2 + out_height / 2)\n",
    "\n",
    "    # tests if the output bbox coordinates are out of frame limits\n",
    "    if startX_out < 0:\n",
    "        startX_out = 0\n",
    "    if endX_out > int(w):\n",
    "        endX_out = int(w)\n",
    "    if startY_out < 0:\n",
    "        startY_out = 0\n",
    "    if endY_out > int(h):\n",
    "        endY_out = int(h)\n",
    "    return [startX_out, startY_out, endX_out, endY_out]\n",
    "\n",
    "\n",
    "@patch\n",
    "def extract_faces(\n",
    "    self: ForensicFace,\n",
    "    video_path: str,  # path to video file\n",
    "    dest_folder: str = None,  # folder used to save extracted faces. If not provided, a new folder with the video name is created\n",
    "    every_n_frames: int = 1,  # skip some frames\n",
    "    margin: float = 2.0,  # margin to add to each face, w.r.t. detected bounding box\n",
    "    start_from: float = 0.0,  # seconds after video start to begin processing\n",
    "):\n",
    "    if dest_folder is None:\n",
    "        dest_folder = os.path.splitext(video_path)[0]\n",
    "\n",
    "    os.makedirs(dest_folder, exist_ok=True)\n",
    "\n",
    "    # initialize video stream from file\n",
    "    vs = cv2.VideoCapture(video_path)\n",
    "    fps = vs.get(cv2.CAP_PROP_FPS)\n",
    "    start_frame = int(fps * start_from)\n",
    "\n",
    "    # seek to starting frame\n",
    "    vs.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "    current_frame = start_frame\n",
    "    nfaces = 0\n",
    "    while True:\n",
    "\n",
    "        if (current_frame % every_n_frames) != 0:\n",
    "            current_frame = current_frame + 1\n",
    "            vs.set(cv2.CAP_PROP_POS_FRAMES, current_frame)\n",
    "            continue\n",
    "\n",
    "        ret, frame = vs.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "        current_frame = current_frame + 1\n",
    "        (h, w) = frame.shape[:2]\n",
    "\n",
    "        faces = self.detectmodel.get(frame)\n",
    "        for i, face in enumerate(faces):\n",
    "            startX, startY, endX, endY = face.bbox.astype(\"int\")\n",
    "            faceW = endX - startX\n",
    "            faceH = endY - startY\n",
    "            outBbox = self._get_extended_bbox(\n",
    "                face.bbox, frame.shape, margin_factor=margin\n",
    "            )\n",
    "            # export the face (with added margin)\n",
    "            face_crop = frame[outBbox[1] : outBbox[3], outBbox[0] : outBbox[2]]\n",
    "            face_img_path = os.path.join(\n",
    "                dest_folder, f\"frame_{current_frame:07}_face_{i:02}.png\"\n",
    "            )\n",
    "            cv2.imwrite(face_img_path, face_crop)\n",
    "            nfaces += 1\n",
    "    vs.release()\n",
    "    return nfaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = ForensicFace()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff.extract_faces(\n",
    "    video_path=\"/home/rafael/productionID_3762907.mp4\",\n",
    "    start_from=0,\n",
    "    every_n_frames=10,\n",
    "    dest_folder=\"/home/rafael/video_faces\",\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "onnxgpu",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
