{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# forensicface--A tool for forensic face examination\n",
    "\n",
    "> An integrated tool to compare faces using state-of-the-art face recognition models and compute Likelihood Ratios "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.utils import *\n",
    "import onnxruntime\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "from glob import glob\n",
    "from imutils import build_montages\n",
    "from insightface.app import FaceAnalysis\n",
    "from insightface.utils import face_align\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from forensicface.utils import freeze_env, transform_keypoints, annotate_img_with_kps\n",
    "import warnings\n",
    "\n",
    "\n",
    "def custom_formatwarning(message, category, filename, lineno, line=None):\n",
    "    return f\"{category.__name__}: {message}\\n\"\n",
    "\n",
    "\n",
    "warnings.formatwarning = custom_formatwarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class ForensicFace:\n",
    "    \"\"\"\n",
    "    Class for processing facial images to extract useful features for forensic analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    IMG_SIZE = (112, 112)\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        models: list[str] = [\"sepaelv2\"],\n",
    "        model: str = None,\n",
    "        det_size: int = 320,\n",
    "        use_gpu: bool = True,\n",
    "        gpu: int = 0,  # which GPU to use\n",
    "        concat_embeddings: bool = True,\n",
    "        extended=True,\n",
    "        det_thresh: float = 0.5,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        A face comparison tool for forensic analysis and comparison of facial images.\n",
    "\n",
    "        Args:\n",
    "        - models (list[str]): The names of the face recognition models to use (default: [\"sepaelv2\"]).\n",
    "        - model (str): [deprecated] The name of the face recognition model to use\n",
    "        - det_size (int): The size of the input images for face detection (default: 320).\n",
    "        - use_gpu (bool): Whether to use a GPU for inference (default: True).\n",
    "        - gpu (int): The ID of the GPU to use (default: 0).\n",
    "        - concat_embeddings (bool): If True, concatenates the embeddings of each model.\n",
    "        - extended (bool): Whether to use extended modules (detection, landmark_3d_68, genderage) (default: True).\n",
    "        - det_thresh (float): threshold for the face detector (default = 0.5).\n",
    "        \"\"\"\n",
    "\n",
    "        if model is not None:\n",
    "            warnings.warn(\n",
    "                \"__init__: The 'model' parameter is deprecated and will be removed in a future release.\\n\"\n",
    "                \"Please use the 'models' parameter instead: models = ['model_name']\",\n",
    "                DeprecationWarning,\n",
    "            )\n",
    "            self.models = [model]\n",
    "        else:\n",
    "            self.models = models\n",
    "        self.rec_inference_sessions = [\n",
    "            self._load_model(model_name, use_gpu, gpu) for model_name in self.models\n",
    "        ]\n",
    "\n",
    "        self.det_size = (det_size, det_size)\n",
    "        self.det_thresh = det_thresh\n",
    "        self.extended = extended\n",
    "        if self.extended:\n",
    "            allowed_modules = [\"detection\", \"landmark_3d_68\", \"genderage\"]\n",
    "\n",
    "            self.ort_fiqa = onnxruntime.InferenceSession(\n",
    "                osp.join(\n",
    "                    osp.expanduser(\"~/.insightface/models\"),\n",
    "                    self.models[0],\n",
    "                    \"cr_fiqa\",\n",
    "                    \"cr_fiqa_l.onnx\",\n",
    "                ),\n",
    "                providers=(\n",
    "                    [(\"CUDAExecutionProvider\", {\"device_id\": gpu})]\n",
    "                    if use_gpu\n",
    "                    else [\"CPUExecutionProvider\"]\n",
    "                ),\n",
    "            )\n",
    "        else:\n",
    "            allowed_modules = [\"detection\"]\n",
    "\n",
    "        self.detectmodel = FaceAnalysis(\n",
    "            name=self.models[0],\n",
    "            allowed_modules=allowed_modules,\n",
    "            providers=(\n",
    "                [(\"CUDAExecutionProvider\", {\"device_id\": gpu})]\n",
    "                if use_gpu\n",
    "                else [\"CPUExecutionProvider\"]\n",
    "            ),\n",
    "        )\n",
    "        self.detectmodel.prepare(\n",
    "            ctx_id=gpu if use_gpu else -1,\n",
    "            det_size=self.det_size,\n",
    "            det_thresh=self.det_thresh,\n",
    "        )\n",
    "\n",
    "        self.environment = freeze_env()\n",
    "        self.concat_embeddings = concat_embeddings\n",
    "\n",
    "    def _load_model(self, model_name, use_gpu, gpu):\n",
    "        \"\"\"Loads a single ONNX model.\"\"\"\n",
    "        model_path = glob(\n",
    "            osp.join(\n",
    "                osp.expanduser(\"~/.insightface/models\"), model_name, \"*\", \"*face*.onnx\"\n",
    "            )\n",
    "        )\n",
    "        assert len(model_path) == 1, f\"More than one ONNX model found in {model_path}\"\n",
    "        return onnxruntime.InferenceSession(\n",
    "            model_path[0],\n",
    "            providers=(\n",
    "                [(\"CUDAExecutionProvider\", {\"device_id\": gpu})]\n",
    "                if use_gpu\n",
    "                else [\"CPUExecutionProvider\"]\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def _to_input_ada(self, aligned_bgr_img):\n",
    "        \"\"\"\n",
    "        Preprocesses the input face for the face recognition model.\n",
    "\n",
    "        Args:\n",
    "            face: Face image as a numpy array in BGR order.\n",
    "\n",
    "        Returns:\n",
    "            Preprocessed face image as a numpy array.\n",
    "        \"\"\"\n",
    "        _aligned_bgr_img = aligned_bgr_img.astype(np.float32)\n",
    "        _aligned_bgr_img = ((_aligned_bgr_img / 255.0) - 0.5) / 0.5\n",
    "        return _aligned_bgr_img.transpose(2, 0, 1).reshape(1, 3, *self.IMG_SIZE)\n",
    "\n",
    "    def _get_best_face(self, img, faces, criterion=\"size\"):\n",
    "        \"\"\"Get the best face based on a criterion: 'centrality' or 'size'.\"\"\"\n",
    "        assert criterion in [\"centrality\", \"size\"]\n",
    "        assert faces is not None and len(faces) > 0\n",
    "\n",
    "        if criterion == \"centrality\":\n",
    "            img_center = np.array([img.shape[0] // 2, img.shape[1] // 2])\n",
    "            scores = [\n",
    "                np.linalg.norm(\n",
    "                    img_center\n",
    "                    - np.array([(box[0] + box[2]) // 2, (box[1] + box[3]) // 2])\n",
    "                )\n",
    "                for box in [face.bbox.astype(\"int\").flatten() for face in faces]\n",
    "            ]\n",
    "        elif criterion == \"size\":\n",
    "            scores = [\n",
    "                abs((box[2] - box[0]) * (box[3] - box[1]))\n",
    "                for box in [face.bbox.astype(\"int\").flatten() for face in faces]\n",
    "            ]\n",
    "\n",
    "        if criterion == \"centrality\":\n",
    "            best_idx = scores.index(min(scores))\n",
    "        else:\n",
    "            best_idx = scores.index(max(scores))\n",
    "\n",
    "        return faces[best_idx]\n",
    "\n",
    "    def process_image_single_face(\n",
    "        self, imgpath: str, draw_keypoints=False\n",
    "    ):  # Path to image to be processed\n",
    "        \"\"\"\n",
    "        Process a an image considering it has a single face and extract useful features for forensic analysis.\n",
    "        If more than one face is detected, the largest face will be returned.\n",
    "        THIS METHOD IS DEPRECATED AND WILL BE REMOVED IN A FUTURE RELEASE. Use process_image instead.\n",
    "        \"\"\"\n",
    "        warnings.warn(\n",
    "            \"process_image_single_face: This method is deprecated and will be removed in a future release.\\n\"\n",
    "            \"Use the 'process_image' method instead. \",\n",
    "            DeprecationWarning,\n",
    "        )\n",
    "        bgr_img = self._load_image(imgpath)\n",
    "        return self.process_image(\n",
    "            bgr_img,\n",
    "            draw_keypoints=draw_keypoints,\n",
    "            single_face=True,\n",
    "            select_single_face_by=\"size\",\n",
    "        )\n",
    "\n",
    "    def process_image(\n",
    "        self,\n",
    "        imgpath,\n",
    "        single_face=True,\n",
    "        draw_keypoints=False,\n",
    "        select_single_face_by=\"size\",\n",
    "    ):\n",
    "        \"\"\"Process an image assuming one or multiple faces.\n",
    "        Args:\n",
    "            - imgpath (str | np.ndarray): Path to the input image or cv2 image array in BGR.\n",
    "            - draw_keypoints (bool): If set to True, draw the keypoints on the aligned face.\n",
    "            - single_face (bool): If set to True, assume the image may contain more than one face.\n",
    "            - select_single_face_by (str): criterion to select the face in the image, if more than one face is detected.\n",
    "                Only applicable when single_face == True. Must be either 'size' or 'centrality'.\n",
    "        Returns:\n",
    "            A dictionary containing the following keys:\n",
    "                - 'keypoints': A 2D numpy array of shape (5, 2) containing the facial keypoints\n",
    "                        for each face in the image. The keypoints are ordered as follows:\n",
    "                       left eye, right eye, nose tip, left mouth corner, and right mouth corner.\n",
    "\n",
    "                - 'ipd': A float representing the inter-pupillary distance for each face in the image.\n",
    "\n",
    "                - 'embedding': A 1D numpy array of shape (512,) containing the facial embedding\n",
    "                       for each face in the image.\n",
    "                       If the concat_emmbeddings == True, keys for each model are used with the names <model_name>_embedding\n",
    "\n",
    "                - 'bbox': A 1D numpy array of shape (4,) containing the bounding box coordinates for each face\n",
    "                  in the image. The coordinates are ordered as follows: (xmin, ymin, xmax, ymax).\n",
    "\n",
    "                - 'aligned_face': A 3D numpy array of shape (H, W, C) in RGB order containing the aligned face image for\n",
    "                          each face in the image. The image has been cropped and aligned based on the\n",
    "                          facial keypoints.\n",
    "\n",
    "                - 'det_score': A float representing the face detection score.\n",
    "\n",
    "                If the 'extended' is set to True, the dictionary will also contain the following keys:\n",
    "                - 'gender': A string representing the gender for each face in the image.\n",
    "                               Possible values are 'M' for male and 'F' for female.\n",
    "\n",
    "                - 'age': An integer representing the estimated age for each face in the image.\n",
    "\n",
    "                - 'pitch': A float representing the pitch angle for each face in the image.\n",
    "\n",
    "                - 'yaw': A float representing the yaw angle for each face in the image.\n",
    "\n",
    "                - 'roll': A float representing the roll angle for each face in the image.\n",
    "\n",
    "                - fiqa_score: A float indicating facial image quality.\n",
    "        \"\"\"\n",
    "        if single_face == True:\n",
    "            warnings.warn(\n",
    "                \"process_image: The return of this function when 'single_face = True' will change in a future release.\\n\"\n",
    "                \"Instead of returning a dict, it will return a list (with one dict). \",\n",
    "                FutureWarning,\n",
    "            )\n",
    "        bgr_img = self._load_image(imgpath)\n",
    "        faces = self.detectmodel.get(bgr_img)\n",
    "        if len(faces) == 0:\n",
    "            return []\n",
    "\n",
    "        if single_face:\n",
    "            faces = [\n",
    "                self._get_best_face(bgr_img, faces, criterion=select_single_face_by)\n",
    "            ]\n",
    "\n",
    "        results = []\n",
    "        for face in faces:\n",
    "            bgr_aligned_face = face_align.norm_crop(bgr_img, face.kps)\n",
    "            embeddings, fiqa_score = self._compute_embeddings(bgr_aligned_face)\n",
    "            if draw_keypoints:\n",
    "                bgr_aligned_face = self._draw_keypoints_on_aligned_face(\n",
    "                    bgr_aligned_face, face.kps\n",
    "                )\n",
    "            result = self._assemble_result(\n",
    "                face, bgr_aligned_face, embeddings, fiqa_score\n",
    "            )\n",
    "            results.append(result)\n",
    "\n",
    "        return results if not single_face else results[0]\n",
    "\n",
    "    def _draw_keypoints_on_aligned_face(self, bgr_aligned_face, keypoints):\n",
    "        aligned_face = bgr_aligned_face.copy()\n",
    "        M = face_align.estimate_norm(keypoints)\n",
    "        aligned_kps = transform_keypoints(keypoints=keypoints, M=M)\n",
    "        annotated_aligned_face = annotate_img_with_kps(\n",
    "            aligned_face, kps=aligned_kps, color=\"green\"\n",
    "        )\n",
    "        return annotated_aligned_face\n",
    "\n",
    "    def _compute_embeddings(self, bgr_aligned_face):\n",
    "        \"\"\"Computes embeddings and FIQA score for an aligned face.\"\"\"\n",
    "        img_to_input = self._to_input_ada(bgr_aligned_face)\n",
    "        embeddings = []\n",
    "        for rec_ort in self.rec_inference_sessions:\n",
    "            model_inputs = {rec_ort.get_inputs()[0].name: img_to_input}\n",
    "            model_output = rec_ort.run(None, model_inputs)\n",
    "            if (\n",
    "                len(model_output) == 2\n",
    "            ):  # model output in the form of normed_embedding, norm\n",
    "                embedding = model_output[0].flatten() * model_output[1].flatten()[0]\n",
    "            else:  # model output in the form of embedding\n",
    "                embedding = model_output[0].flatten()\n",
    "            embeddings.append(embedding)\n",
    "\n",
    "        fiqa_score = None\n",
    "        if self.extended:\n",
    "            _, fiqa_score = self.ort_fiqa.run(\n",
    "                None, {self.ort_fiqa.get_inputs()[0].name: img_to_input}\n",
    "            )\n",
    "\n",
    "        return (\n",
    "            np.concatenate(embeddings) if self.concat_embeddings else embeddings,\n",
    "            fiqa_score[0][0] if fiqa_score is not None else None,\n",
    "        )\n",
    "\n",
    "    def _assemble_result(self, face, bgr_aligned_face, embeddings, fiqa_score):\n",
    "        \"\"\"Assembles the result dictionary for a face.\"\"\"\n",
    "        ret = {\n",
    "            \"ipd\": np.linalg.norm(face.kps[0] - face.kps[1]),\n",
    "        }\n",
    "\n",
    "        if self.extended:\n",
    "            ret.update(\n",
    "                {\n",
    "                    \"fiqa_score\": fiqa_score,\n",
    "                    \"gender\": \"M\" if face.gender == 1 else \"F\",\n",
    "                    \"age\": face.age,\n",
    "                    \"yaw\": face.pose[1],\n",
    "                    \"pitch\": face.pose[0],\n",
    "                    \"roll\": face.pose[2],\n",
    "                }\n",
    "            )\n",
    "\n",
    "        ret.update(\n",
    "            {\n",
    "                \"det_score\": face.det_score,\n",
    "                \"keypoints\": face.kps,\n",
    "                \"bbox\": face.bbox.astype(\"int\"),\n",
    "            }\n",
    "        )\n",
    "        if self.concat_embeddings:\n",
    "            ret[\"embedding\"] = embeddings\n",
    "        else:\n",
    "            for model_name, embedding in zip(self.models, embeddings):\n",
    "                ret[f\"embedding_{model_name}\"] = embedding\n",
    "\n",
    "        ret[\"aligned_face\"] = cv2.cvtColor(bgr_aligned_face, cv2.COLOR_BGR2RGB)\n",
    "        return ret\n",
    "\n",
    "    def _load_image(self, imgpath):\n",
    "        \"\"\"Load image from file path or return the array if already loaded.\"\"\"\n",
    "        return cv2.imread(imgpath) if isinstance(imgpath, str) else imgpath.copy()\n",
    "\n",
    "    def process_image_multiple_faces(\n",
    "        self, imgpath: str, draw_keypoints=False  # Path to image to be processed\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Process an image with one or multiple faces.\n",
    "        THIS METHOD IS DEPRECATED AND WILL BE REMOVED IN A FUTURE RELEASE. Use process_image instead.\n",
    "        \"\"\"\n",
    "        warnings.warn(\n",
    "            \"process_image_multiple_faces: This method is deprecated and will be removed in a future release.\\n\"\n",
    "            \"Use the 'process_image' method instead.\",\n",
    "            DeprecationWarning,\n",
    "        )\n",
    "        bgr_img = self._load_image(imgpath)\n",
    "        return self.process_image(\n",
    "            bgr_img, draw_keypoints=draw_keypoints, single_face=False\n",
    "        )\n",
    "\n",
    "    def build_mosaic(\n",
    "        self,\n",
    "        img_path_list,\n",
    "        mosaic_shape,\n",
    "        border=0.03,\n",
    "        save_to=None,\n",
    "        draw_keypoints=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Build a rectangular mosaic of the aligned faces.\n",
    "        Based on the imutils build_montages function.\n",
    "\n",
    "        Parameters:\n",
    "            img_path_list: list of paths to image files or list of bgr_images\n",
    "            mosaic_shape: tuple of integers, (n_cols, n_rows)\n",
    "            border: float, percent of image to use as white border\n",
    "\n",
    "        Returns:\n",
    "            cv2 BGR image with mosaic\n",
    "        \"\"\"\n",
    "        assert mosaic_shape is not None\n",
    "        top = int(border * self.IMG_SIZE[0])  # shape[0] = rows\n",
    "        bottom = top\n",
    "        left = int(border * self.IMG_SIZE[1])  # shape[1] = cols\n",
    "        right = left\n",
    "\n",
    "        imgs = []\n",
    "        list_of_arrays = False\n",
    "        for img in img_path_list:\n",
    "            if type(img) != str:  # image array passed as argument\n",
    "                list_of_arrays = True\n",
    "            ret = self.process_image(\n",
    "                img, draw_keypoints=draw_keypoints, single_face=True\n",
    "            )\n",
    "            if len(ret) > 0:\n",
    "                img = cv2.cvtColor(ret[\"aligned_face\"], cv2.COLOR_RGB2BGR)\n",
    "                img = cv2.copyMakeBorder(\n",
    "                    img,\n",
    "                    top=top,\n",
    "                    bottom=bottom,\n",
    "                    left=left,\n",
    "                    right=right,\n",
    "                    borderType=cv2.BORDER_CONSTANT,\n",
    "                    value=(255, 255, 255),\n",
    "                )\n",
    "                imgs.append(img)\n",
    "        mosaic = build_montages(\n",
    "            imgs,\n",
    "            image_shape=(\n",
    "                int(self.IMG_SIZE[0] * (1 + 2 * border)),\n",
    "                int(self.IMG_SIZE[1] * (1 + 2 * border)),\n",
    "            ),\n",
    "            montage_shape=mosaic_shape,\n",
    "        )[0]\n",
    "        if list_of_arrays:\n",
    "            warnings.warn(\n",
    "                \"A list of arrays was passed as argument. Make sure image arrays are in BGR format.\",\n",
    "                Warning,\n",
    "            )\n",
    "        if save_to is not None:\n",
    "            cv2.imwrite(save_to, mosaic)\n",
    "        return mosaic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'device_id': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'cudnn_conv_use_max_workspace': '1', 'tunable_op_enable': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0'}}\n",
      "find model: /home/rafael/.insightface/models/sepaelv2/1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'device_id': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'cudnn_conv_use_max_workspace': '1', 'tunable_op_enable': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0'}}\n",
      "find model: /home/rafael/.insightface/models/sepaelv2/det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'device_id': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'cudnn_conv_use_max_workspace': '1', 'tunable_op_enable': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0'}}\n",
      "find model: /home/rafael/.insightface/models/sepaelv2/genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "set det-size: (320, 320)\n"
     ]
    }
   ],
   "source": [
    "ff = ForensicFace(\n",
    "    use_gpu=True,\n",
    "    extended=True,\n",
    "    det_thresh=0.5,\n",
    "    models=[\"sepaelv2\", \"sepaelv4\"],\n",
    "    concat_embeddings=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<onnxruntime.capi.onnxruntime_inference_collection.InferenceSession>,\n",
       " <onnxruntime.capi.onnxruntime_inference_collection.InferenceSession>]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff.rec_inference_sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DeprecationWarning: process_image_single_face: This method is deprecated and will be removed in a future release.\n",
      "Use the 'process_image' method instead. \n",
      "FutureWarning: process_image: The return of this function when 'single_face = True' will change in a future release.\n",
      "Instead of returning a dict, it will return a list (with one dict). \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['ipd', 'fiqa_score', 'gender', 'age', 'yaw', 'pitch', 'roll', 'det_score', 'keypoints', 'bbox', 'embedding_sepaelv2', 'embedding_sepaelv4', 'aligned_face'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret = ff.process_image_single_face(\"obama.png\")\n",
    "ret.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DeprecationWarning: process_image_multiple_faces: This method is deprecated and will be removed in a future release.\n",
      "Use the 'process_image' method instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8,\n",
       " dict_keys(['ipd', 'fiqa_score', 'gender', 'age', 'yaw', 'pitch', 'roll', 'det_score', 'keypoints', 'bbox', 'embedding_sepaelv2', 'embedding_sepaelv4', 'aligned_face']),\n",
       " dict_keys(['ipd', 'fiqa_score', 'gender', 'age', 'yaw', 'pitch', 'roll', 'det_score', 'keypoints', 'bbox', 'embedding_sepaelv2', 'embedding_sepaelv4', 'aligned_face']))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret = ff.process_image_multiple_faces(\"tela.png\")\n",
    "len(ret), ret[0].keys(), ret[1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ipd', 'fiqa_score', 'gender', 'age', 'yaw', 'pitch', 'roll', 'det_score', 'keypoints', 'bbox', 'embedding_sepaelv2', 'embedding_sepaelv4', 'aligned_face'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret = ff.process_image(\"obama.png\", single_face=False)\n",
    "ret[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'device_id': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'cudnn_conv_use_max_workspace': '1', 'tunable_op_enable': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0'}}\n",
      "model ignore: /home/rafael/.insightface/models/sepaelv2/1k3d68.onnx landmark_3d_68\n",
      "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'device_id': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'cudnn_conv_use_max_workspace': '1', 'tunable_op_enable': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0'}}\n",
      "find model: /home/rafael/.insightface/models/sepaelv2/det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'device_id': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'cudnn_conv_use_max_workspace': '1', 'tunable_op_enable': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0'}}\n",
      "model ignore: /home/rafael/.insightface/models/sepaelv2/genderage.onnx genderage\n",
      "set det-size: (320, 320)\n"
     ]
    }
   ],
   "source": [
    "ff = ForensicFace(\n",
    "    use_gpu=True,\n",
    "    extended=False,\n",
    "    det_thresh=0.5,\n",
    "    models=[\"sepaelv2\"],\n",
    "    concat_embeddings=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DeprecationWarning: process_image_single_face: This method is deprecated and will be removed in a future release.\n",
      "Use the 'process_image' method instead. \n",
      "FutureWarning: process_image: The return of this function when 'single_face = True' will change in a future release.\n",
      "Instead of returning a dict, it will return a list (with one dict). \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['ipd', 'det_score', 'keypoints', 'bbox', 'embedding', 'aligned_face'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret = ff.process_image_single_face(\"obama.png\")\n",
    "ret.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512,)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret[\"embedding\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Python version': '3.10.11 (main, May 16 2023, 00:28:57) [GCC 11.2.0]',\n",
       " 'albumentations': '1.3.1',\n",
       " 'asttokens': '2.2.1',\n",
       " 'astunparse': '1.6.3',\n",
       " 'backcall': '0.2.0',\n",
       " 'backports.functools-lru-cache': '1.6.4',\n",
       " 'bleach': '6.0.0',\n",
       " 'certifi': '2023.5.7',\n",
       " 'cffi': '1.15.1',\n",
       " 'charset-normalizer': '3.1.0',\n",
       " 'coloredlogs': '15.0.1',\n",
       " 'contourpy': '1.1.0',\n",
       " 'cryptography': '41.0.1',\n",
       " 'cycler': '0.11.0',\n",
       " 'Cython': '0.29.35',\n",
       " 'debugpy': '1.5.1',\n",
       " 'decorator': '5.1.1',\n",
       " 'docutils': '0.20.1',\n",
       " 'easydict': '1.10',\n",
       " 'entrypoints': '0.4',\n",
       " 'execnb': '0.1.5',\n",
       " 'executing': '1.2.0',\n",
       " 'fastcore': '1.5.29',\n",
       " 'flatbuffers': '23.5.26',\n",
       " 'fonttools': '4.40.0',\n",
       " 'forensicface': '0.3.6',\n",
       " 'ghapi': '1.0.4',\n",
       " 'humanfriendly': '10.0',\n",
       " 'idna': '3.4',\n",
       " 'imageio': '2.31.1',\n",
       " 'importlib-metadata': '6.7.0',\n",
       " 'imutils': '0.5.4',\n",
       " 'insightface': '0.7.3',\n",
       " 'ipykernel': '6.15.0',\n",
       " 'ipython': '8.14.0',\n",
       " 'jaraco.classes': '3.2.3',\n",
       " 'jedi': '0.18.2',\n",
       " 'jeepney': '0.8.0',\n",
       " 'joblib': '1.2.0',\n",
       " 'jupyter-client': '7.3.4',\n",
       " 'jupyter-core': '5.3.1',\n",
       " 'jupyterlab-quarto': '0.2.8',\n",
       " 'keyring': '24.0.0',\n",
       " 'kiwisolver': '1.4.4',\n",
       " 'lazy-loader': '0.2',\n",
       " 'markdown-it-py': '3.0.0',\n",
       " 'matplotlib': '3.7.1',\n",
       " 'matplotlib-inline': '0.1.6',\n",
       " 'mdurl': '0.1.2',\n",
       " 'more-itertools': '9.1.0',\n",
       " 'mpmath': '1.3.0',\n",
       " 'nbdev': '2.3.27',\n",
       " 'nest-asyncio': '1.5.6',\n",
       " 'networkx': '3.1',\n",
       " 'numpy': '1.25.0',\n",
       " 'onnx': '1.14.0',\n",
       " 'onnxruntime-gpu': '1.15.0',\n",
       " 'opencv-python-headless': '4.7.0.72',\n",
       " 'packaging': '23.1',\n",
       " 'pandas': '2.0.2',\n",
       " 'parso': '0.8.3',\n",
       " 'pexpect': '4.8.0',\n",
       " 'pickleshare': '0.7.5',\n",
       " 'Pillow': '9.5.0',\n",
       " 'pip': '23.1.2',\n",
       " 'pkginfo': '1.9.6',\n",
       " 'platformdirs': '3.6.0',\n",
       " 'prettytable': '3.8.0',\n",
       " 'prompt-toolkit': '3.0.38',\n",
       " 'protobuf': '4.23.3',\n",
       " 'psutil': '5.9.0',\n",
       " 'ptyprocess': '0.7.0',\n",
       " 'pure-eval': '0.2.2',\n",
       " 'pycparser': '2.21',\n",
       " 'Pygments': '2.15.1',\n",
       " 'pyparsing': '3.1.0',\n",
       " 'python-dateutil': '2.8.2',\n",
       " 'pytz': '2023.3',\n",
       " 'PyWavelets': '1.4.1',\n",
       " 'PyYAML': '6.0',\n",
       " 'pyzmq': '25.1.0',\n",
       " 'qudida': '0.0.4',\n",
       " 'readme-renderer': '40.0',\n",
       " 'requests': '2.31.0',\n",
       " 'requests-toolbelt': '1.0.0',\n",
       " 'rfc3986': '2.0.0',\n",
       " 'rich': '13.4.2',\n",
       " 'scikit-image': '0.21.0',\n",
       " 'scikit-learn': '1.2.2',\n",
       " 'scipy': '1.10.1',\n",
       " 'seaborn': '0.13.2',\n",
       " 'SecretStorage': '3.3.3',\n",
       " 'setuptools': '67.8.0',\n",
       " 'six': '1.16.0',\n",
       " 'stack-data': '0.6.2',\n",
       " 'sympy': '1.12',\n",
       " 'threadpoolctl': '3.1.0',\n",
       " 'tifffile': '2023.4.12',\n",
       " 'tornado': '6.1',\n",
       " 'tqdm': '4.65.0',\n",
       " 'traitlets': '5.9.0',\n",
       " 'twine': '4.0.2',\n",
       " 'typing-extensions': '4.6.3',\n",
       " 'tzdata': '2023.3',\n",
       " 'urllib3': '2.0.3',\n",
       " 'watchdog': '3.0.0',\n",
       " 'wcwidth': '0.2.6',\n",
       " 'webencodings': '0.5.1',\n",
       " 'wheel': '0.38.4',\n",
       " 'zipp': '3.15.0'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff.environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: process_image: The return of this function when 'single_face = True' will change in a future release.\n",
      "Instead of returning a dict, it will return a list (with one dict). \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(dict_keys(['ipd', 'fiqa_score', 'gender', 'age', 'yaw', 'pitch', 'roll', 'det_score', 'keypoints', 'bbox', 'embedding_sepaelv2', 'embedding_sepaelv4', 'aligned_face']),\n",
       " array([[ 61.428093,  87.567154],\n",
       "        [103.14688 ,  97.62415 ],\n",
       "        [ 61.404076, 114.31358 ],\n",
       "        [ 50.038876, 143.41814 ],\n",
       "        [ 82.59338 , 152.32835 ]], dtype=float32),\n",
       " 42.91387,\n",
       " (512,),\n",
       " 0.83124155)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = ff.process_image(\"obama2.png\", single_face=True, draw_keypoints=True)\n",
    "result.keys(), result[\"keypoints\"], result[\"ipd\"], result[\"embedding\"].shape, result[\n",
    "    \"det_score\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: process_image: The return of this function when 'single_face = True' will change in a future release.\n",
      "Instead of returning a dict, it will return a list (with one dict). \n",
      "Warning: A list of arrays was passed as argument. Make sure image arrays are in BGR format.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(118, 236, 3)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs = [cv2.imread(x) for x in [\"001_cam1_1.jpg\", \"001_frontal.jpg\"]]\n",
    "mosaic = ff.build_mosaic(imgs, mosaic_shape=(2, 1))\n",
    "mosaic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DeprecationWarning: process_image_multiple_faces: This method is deprecated and will be removed in a future release.\n",
      "Use the 'process_image' method instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(dict_keys(['ipd', 'det_score', 'keypoints', 'bbox', 'embedding', 'aligned_face']),\n",
       " array([[471.42743, 418.60498],\n",
       "        [522.68933, 418.05362],\n",
       "        [498.82196, 449.08923],\n",
       "        [479.3499 , 476.4419 ],\n",
       "        [514.33453, 476.06885]], dtype=float32),\n",
       " array([441, 355, 548, 506]),\n",
       " 0.8962144)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = ff.process_image_multiple_faces(\"tela.png\", draw_keypoints=True)\n",
    "results[0].keys(), results[0][\"keypoints\"], results[0][\"bbox\"], results[0][\"det_score\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparação entre duas imagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "@patch\n",
    "def compare(self: ForensicFace, img1path: str, img2path: str):\n",
    "    \"\"\"\n",
    "    Compares the similarity between two face images based on their embeddings.\n",
    "\n",
    "    Parameters:\n",
    "        - img1path (str): Path to the first image file\n",
    "        - img2path (str): Path to the second image file\n",
    "\n",
    "    Returns:\n",
    "        A float representing the similarity score between the two faces based on their embeddings.\n",
    "        The score ranges from -1.0 to 1.0, where 1.0 represents a perfect match and -1.0 represents a complete mismatch.\n",
    "    \"\"\"\n",
    "    img1data = self.process_image(img1path, single_face=True)\n",
    "    assert len(img1data) > 0, f\"No face detected in {img1path}\"\n",
    "    img2data = self.process_image(img2path, single_face=True)\n",
    "    assert len(img2data) > 0, f\"No face detected in {img2path}\"\n",
    "    assert self.concat_embeddings == True\n",
    "\n",
    "    return np.dot(img1data[\"embedding\"], img2data[\"embedding\"]) / (\n",
    "        np.linalg.norm(img1data[\"embedding\"]) * np.linalg.norm(img2data[\"embedding\"])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: process_image: The return of this function when 'single_face = True' will change in a future release.\n",
      "Instead of returning a dict, it will return a list (with one dict). \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8556093"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff.compare(\"obama.png\", \"obama2.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agregação de embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "@patch\n",
    "def aggregate_embeddings(self: ForensicFace, embeddings, weights=None, method=\"mean\"):\n",
    "    \"\"\"\n",
    "    Aggregates multiple embeddings into a single embedding.\n",
    "\n",
    "    Args:\n",
    "        embeddings (numpy.ndarray): A 2D array of shape (num_embeddings, embedding_dim) containing the embeddings to be\n",
    "            aggregated.\n",
    "        weights (numpy.ndarray, optional): A 1D array of shape (num_embeddings,) containing the weights to be assigned\n",
    "            to each embedding. If not provided, all embeddings are equally weighted.\n",
    "\n",
    "        method (str, optional): choice of agregating based on the mean or median of the embeddings. Possible values are\n",
    "            'mean' and 'median'.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: A 1D array of shape (embedding_dim,) containing the aggregated embedding.\n",
    "    \"\"\"\n",
    "    if weights is None:\n",
    "        weights = np.ones(embeddings.shape[0], dtype=\"int\")\n",
    "    assert embeddings.shape[0] == weights.shape[0]\n",
    "    assert method in [\"mean\", \"median\"]\n",
    "    if method == \"mean\":\n",
    "        return np.average(embeddings, axis=0, weights=weights)\n",
    "    else:\n",
    "        weighted_embeddings = np.array([w * e for w, e in zip(weights, embeddings)])\n",
    "        return np.median(weighted_embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "@patch\n",
    "def aggregate_from_images(\n",
    "    self: ForensicFace, list_of_image_paths, method=\"mean\", quality_weight=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Given a list of image paths, this method returns the average embedding of all faces found in the images.\n",
    "\n",
    "    Args:\n",
    "        list_of_image_paths (List[str]): List of paths to images.\n",
    "        method (str, optional): choice of agregating based on the mean or median of the embeddings. Possible values are\n",
    "            'mean' and 'median'.\n",
    "        quality_weight (boolean, optional): If True, use the FIQA(L) score as a weight for aggregation.\n",
    "\n",
    "    Returns:\n",
    "        Union[np.ndarray, List]: If one or more faces are found, returns a 1D numpy array of shape (512,) representing the\n",
    "        average embedding. Otherwise, returns an empty list.\n",
    "    \"\"\"\n",
    "    if quality_weight:\n",
    "        assert (\n",
    "            self.extended == True\n",
    "        ), \"You must initialize ForensicFace with extended = True\"\n",
    "    assert self.concat_embeddings == True\n",
    "    embeddings = []\n",
    "    weights = []\n",
    "    for imgpath in list_of_image_paths:\n",
    "        d = self.process_image(imgpath, single_face=True)\n",
    "        if len(d) > 0:\n",
    "            embeddings.append(d[\"embedding\"])\n",
    "            weights.append(d[\"fiqa_score\"] if quality_weight == True else 1.0)\n",
    "    if len(embeddings) > 0:\n",
    "        return self.aggregate_embeddings(\n",
    "            np.array(embeddings), method=method, weights=np.array(weights)\n",
    "        )\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'device_id': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'cudnn_conv_use_max_workspace': '1', 'tunable_op_enable': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0'}}\n",
      "find model: /home/rafael/.insightface/models/sepaelv2/1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'device_id': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'cudnn_conv_use_max_workspace': '1', 'tunable_op_enable': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0'}}\n",
      "find model: /home/rafael/.insightface/models/sepaelv2/det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'device_id': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'cudnn_conv_use_max_workspace': '1', 'tunable_op_enable': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0'}}\n",
      "find model: /home/rafael/.insightface/models/sepaelv2/genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "set det-size: (320, 320)\n"
     ]
    }
   ],
   "source": [
    "ff = ForensicFace(extended=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: process_image: The return of this function when 'single_face = True' will change in a future release.\n",
      "Instead of returning a dict, it will return a list (with one dict). \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(512,)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated = ff.aggregate_from_images([\"obama.png\", \"obama2.png\"], quality_weight=True)\n",
    "aggregated.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extração de faces de vídeos com margem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch\n",
    "def _get_extended_bbox(self: ForensicFace, bbox, frame_shape, margin_factor):\n",
    "    \"\"\"\n",
    "    Computes and returns the bounding box with extended margins.\n",
    "\n",
    "    Parameters:\n",
    "        bbox (ndarray): The bounding box coordinates (startX, startY, endX, endY).\n",
    "        frame_shape (tuple): The shape of the video frame (height, width, channels).\n",
    "        margin_factor (float): The factor to be applied for computing the margin.\n",
    "\n",
    "    Returns:\n",
    "        A list with the coordinates of the extended bounding box (startX_out, startY_out, endX_out, endY_out).\n",
    "    \"\"\"\n",
    "    # add a margin on the bounding box\n",
    "    (startX, startY, endX, endY) = bbox.astype(\"int\")\n",
    "    (h, w) = frame_shape[:2]\n",
    "    out_width = (endX - startX) * margin_factor\n",
    "    out_height = (endY - startY) * margin_factor\n",
    "\n",
    "    startX_out = int((startX + endX) / 2 - out_width / 2)\n",
    "    endX_out = int((startX + endX) / 2 + out_width / 2)\n",
    "    startY_out = int((startY + endY) / 2 - out_height / 2)\n",
    "    endY_out = int((startY + endY) / 2 + out_height / 2)\n",
    "\n",
    "    # tests if the output bbox coordinates are out of frame limits\n",
    "    if startX_out < 0:\n",
    "        startX_out = 0\n",
    "    if endX_out > int(w):\n",
    "        endX_out = int(w)\n",
    "    if startY_out < 0:\n",
    "        startY_out = 0\n",
    "    if endY_out > int(h):\n",
    "        endY_out = int(h)\n",
    "    return [startX_out, startY_out, endX_out, endY_out]\n",
    "\n",
    "\n",
    "@patch\n",
    "def extract_faces(\n",
    "    self: ForensicFace,\n",
    "    video_path: str,  # path to video file\n",
    "    dest_folder: str = None,  # folder used to save extracted faces. If not provided, a new folder with the video name is created\n",
    "    every_n_frames: int = 1,  # skip some frames\n",
    "    margin: float = 2.0,  # margin to add to each face, w.r.t. detected bounding box\n",
    "    start_from: float = 0.0,  # seconds after video start to begin processing\n",
    "    export_metadata: bool = False,  # if True, export facial keypoints, bounding box, ipd, fiqa_score, pitch, yaw, roll, and embedding\n",
    "):\n",
    "    \"\"\"\n",
    "    Extracts faces from a video and saves them as individual images.\n",
    "\n",
    "    Parameters:\n",
    "        video_path (str): The path to the input video file.\n",
    "        dest_folder (str, optional): The path to the output folder. If not provided, a new folder with the same name as the input video file is created.\n",
    "        every_n_frames (int, optional): Extract faces from every n-th frame. Default is 1 (extract faces from all frames).\n",
    "        margin (float, optional): The factor by which the detected face bounding box should be extended. Default is 2.0.\n",
    "        start_from (float, optional): The time point (in seconds) after which the video frames should be processed. Default is 0.0.\n",
    "\n",
    "    Returns:\n",
    "        The number of extracted faces.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "\n",
    "    if dest_folder is None:\n",
    "        dest_folder = os.path.splitext(video_path)[0]\n",
    "\n",
    "    os.makedirs(dest_folder, exist_ok=True)\n",
    "\n",
    "    # initialize video stream from file\n",
    "    vs = cv2.VideoCapture(video_path)\n",
    "    fps = vs.get(cv2.CAP_PROP_FPS)\n",
    "    start_frame = int(fps * start_from)\n",
    "    total_frames = int(vs.get(cv2.CAP_PROP_FRAME_COUNT)) // every_n_frames\n",
    "\n",
    "    # seek to starting frame\n",
    "    vs.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "    current_frame = start_frame\n",
    "    nfaces = 0\n",
    "    if export_metadata:\n",
    "        metadata = []\n",
    "    with tqdm(\n",
    "        total=total_frames,\n",
    "        bar_format=\"Frames processed: {n}/{total} | Time elapsed: {elapsed}\",\n",
    "    ) as pbar:\n",
    "        while True:\n",
    "\n",
    "            ret, frame = vs.read()\n",
    "\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            current_frame = current_frame + 1\n",
    "            if (current_frame % every_n_frames) != 0:\n",
    "                continue\n",
    "\n",
    "            (h, w) = frame.shape[:2]\n",
    "\n",
    "            # faces = self.detectmodel.get(frame)\n",
    "            rets = self.process_image(frame, single_face=False)\n",
    "            for i, ret in enumerate(rets):\n",
    "                startX, startY, endX, endY = ret[\"bbox\"]\n",
    "                faceW = endX - startX\n",
    "                faceH = endY - startY\n",
    "                outBbox = self._get_extended_bbox(\n",
    "                    ret[\"bbox\"], frame.shape, margin_factor=margin\n",
    "                )\n",
    "                # export the face (with added margin)\n",
    "                face_crop = frame[outBbox[1] : outBbox[3], outBbox[0] : outBbox[2]]\n",
    "                face_img_path = os.path.join(\n",
    "                    dest_folder, f\"frame_{current_frame:07}_face_{i:02}.png\"\n",
    "                )\n",
    "                cv2.imwrite(face_img_path, face_crop)\n",
    "                if export_metadata:\n",
    "                    metadata.append(\n",
    "                        {\n",
    "                            **{\"frame\": current_frame, \"face\": i},\n",
    "                            **{\n",
    "                                k: v\n",
    "                                for k, v in ret.items()\n",
    "                                if k not in [\"det_score\", \"aligned_face\"]\n",
    "                            },\n",
    "                        }\n",
    "                    )\n",
    "                nfaces += 1\n",
    "            pbar.update(1)\n",
    "    vs.release()\n",
    "    if export_metadata:\n",
    "        pd.DataFrame(metadata).to_json(\n",
    "            os.path.join(\n",
    "                dest_folder,\n",
    "                os.path.splitext(os.path.basename(video_path))[0] + \".jsonl\",\n",
    "            ),\n",
    "            lines=True,\n",
    "            orient=\"records\",\n",
    "        )\n",
    "    return nfaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'device_id': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'cudnn_conv_use_max_workspace': '1', 'tunable_op_enable': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0'}}\n",
      "find model: /home/rafael/.insightface/models/sepaelv2/1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'device_id': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'cudnn_conv_use_max_workspace': '1', 'tunable_op_enable': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0'}}\n",
      "find model: /home/rafael/.insightface/models/sepaelv2/det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'device_id': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'cudnn_conv_use_max_workspace': '1', 'tunable_op_enable': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0'}}\n",
      "find model: /home/rafael/.insightface/models/sepaelv2/genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "set det-size: (320, 320)\n"
     ]
    }
   ],
   "source": [
    "ff = ForensicFace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Frames processed: 0/14 | Time elapsed: 00:00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Frames processed: 14/14 | Time elapsed: 00:06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff.extract_faces(\n",
    "    video_path=\"/home/rafael/video/video.mp4\",\n",
    "    start_from=0,\n",
    "    every_n_frames=600,\n",
    "    dest_folder=\"/home/rafael/video_faces\",\n",
    "    export_metadata=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing aligned images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch\n",
    "def process_aligned_face_image(self: ForensicFace, rgb_aligned_face: np.ndarray):\n",
    "    assert rgb_aligned_face.shape == (*self.IMG_SIZE, 3)\n",
    "    bgr_aligned_face = rgb_aligned_face[..., ::-1].copy()\n",
    "    embeddings, fiqa_score = self._compute_embeddings(bgr_aligned_face)\n",
    "\n",
    "    if self.concat_embeddings:\n",
    "        ret = {\"embedding\": embeddings}\n",
    "    else:\n",
    "        ret = {}\n",
    "        for model_name, embedding in zip(self.models, embeddings):\n",
    "            ret[\"embedding_\" + model_name] = embedding\n",
    "    if self.extended:\n",
    "        ret = {**ret, **{\"fiqa_score\": fiqa_score}}\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'device_id': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'cudnn_conv_use_max_workspace': '1', 'tunable_op_enable': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0'}}\n",
      "find model: /home/rafael/.insightface/models/sepaelv2/1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'device_id': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'cudnn_conv_use_max_workspace': '1', 'tunable_op_enable': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0'}}\n",
      "find model: /home/rafael/.insightface/models/sepaelv2/det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'device_id': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'cudnn_conv_use_max_workspace': '1', 'tunable_op_enable': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0'}}\n",
      "find model: /home/rafael/.insightface/models/sepaelv2/genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "set det-size: (320, 320)\n"
     ]
    }
   ],
   "source": [
    "ff = ForensicFace(extended=True, models=[\"sepaelv2\", \"sepaelv4\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DeprecationWarning: process_image_single_face: This method is deprecated and will be removed in a future release.\n",
      "Use the 'process_image' method instead. \n",
      "FutureWarning: process_image: The return of this function when 'single_face = True' will change in a future release.\n",
      "Instead of returning a dict, it will return a list (with one dict). \n"
     ]
    }
   ],
   "source": [
    "ret = ff.process_image_single_face(\"obama.png\")\n",
    "ret2 = ff.process_aligned_face_image(ret[\"aligned_face\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(ret[\"embedding\"], ret2[\"embedding\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024,)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret2[\"embedding\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.1983922, 2.1983922)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret[\"fiqa_score\"], ret2[\"fiqa_score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
