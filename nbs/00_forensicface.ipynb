{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# forensicface--A tool for forensic face examination\n",
    "\n",
    "> An integrated tool to compare faces using state-of-the-art face recognition models and compute Likelihood Ratios "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.utils import *\n",
    "import onnxruntime\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "from insightface.app import FaceAnalysis\n",
    "from insightface.utils import face_align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class ForensicFace:\n",
    "    \"A (forensic) face comparison tool\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, model: str = \"sepaelv2\", det_size: int = 320, use_gpu: bool = True\n",
    "    ):\n",
    "\n",
    "        self.det_size = (det_size, det_size)\n",
    "        self.detectmodel = FaceAnalysis(\n",
    "            name=model,\n",
    "            allowed_modules=[\"detection\"],\n",
    "            providers=[\"CUDAExecutionProvider\"]\n",
    "            if use_gpu\n",
    "            else [\"CPUExecutionProvider\"],\n",
    "        )\n",
    "        self.detectmodel.prepare(ctx_id=0 if use_gpu else -1, det_size=self.det_size)\n",
    "        self.ort_session = onnxruntime.InferenceSession(\n",
    "            osp.join(\n",
    "                osp.expanduser(\"~/.insightface/models\"),\n",
    "                model,\n",
    "                \"adaface\",\n",
    "                \"adaface_ir101web12m.onnx\",\n",
    "            ),\n",
    "            providers=[\"CUDAExecutionProvider\"]\n",
    "            if use_gpu\n",
    "            else [\"CPUExecutionProvider\"],\n",
    "        )\n",
    "\n",
    "    def _to_input(self, aligned_bgr_img):\n",
    "        _aligned_bgr_img = aligned_bgr_img.astype(np.float32)\n",
    "        _aligned_bgr_img = ((_aligned_bgr_img / 255.0) - 0.5) / 0.5\n",
    "        return _aligned_bgr_img.transpose(2, 0, 1).reshape(1, 3, 112, 112)\n",
    "\n",
    "    def get_most_central_face(self, img, faces):\n",
    "        \"\"\"\n",
    "        faces is a insightface object with keypoints and bounding_box\n",
    "\n",
    "        return: keypoints of the most central face\n",
    "        \"\"\"\n",
    "        assert faces is not None\n",
    "        img_center = np.array([img.shape[0] // 2, img.shape[1] // 2])\n",
    "        dist = []\n",
    "\n",
    "        # Compute centers of faces and distances from certer of image\n",
    "        for idx, face in enumerate(faces):\n",
    "            box = face.bbox.astype(\"int\").flatten()\n",
    "            face_center = np.array([(box[0] + box[2]) // 2, (box[1] + box[3]) // 2])\n",
    "            dist.append(np.linalg.norm(img_center - face_center))\n",
    "\n",
    "        # Get index of the face closest to the center of image\n",
    "        return faces[dist.index(min(dist))].kps\n",
    "\n",
    "    def process_image(self, imgpath: str):  # Path to image to be processed\n",
    "        \"\"\"\n",
    "        Process image and returns list of dicts with:\n",
    "\n",
    "        - keypoints: 5 facial points (left eye, right eye, nose tip, left mouth corner and right mouth corner)\n",
    "\n",
    "        - ipd: interpupillary distance\n",
    "\n",
    "        - normalized_embedding\n",
    "\n",
    "        - embedding_norm\n",
    "\n",
    "        - aligned_face: face after alignment using the keypoints as references for affine transform\n",
    "        \"\"\"\n",
    "        bgr_img = cv2.imread(imgpath)\n",
    "        faces = self.detectmodel.get(bgr_img)\n",
    "        if len(faces) == 0:\n",
    "            return {}\n",
    "        kps = self.get_most_central_face(bgr_img, faces)\n",
    "        bgr_aligned_face = face_align.norm_crop(bgr_img, kps)\n",
    "        ipd = np.linalg.norm(kps[0] - kps[1])\n",
    "        ort_inputs = {\n",
    "            self.ort_session.get_inputs()[0].name: self._to_input(bgr_aligned_face)\n",
    "        }\n",
    "        normalized_embedding, norm = self.ort_session.run(None, ort_inputs)\n",
    "        return {\n",
    "            \"keypoints\": kps,\n",
    "            \"ipd\": ipd,\n",
    "            \"embedding\": normalized_embedding.flatten() * norm.flatten()[0],\n",
    "            \"norm\": norm.flatten()[0],\n",
    "            \"aligned_face\": cv2.cvtColor(bgr_aligned_face, cv2.COLOR_BGR2RGB),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'do_copy_in_default_stream': '1', 'arena_extend_strategy': 'kNextPowerOfTwo', 'gpu_external_empty_cache': '0', 'gpu_external_free': '0', 'cudnn_conv_use_max_workspace': '0', 'gpu_mem_limit': '18446744073709551615', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'gpu_external_alloc': '0', 'device_id': '0'}}\n",
      "find model: /home/rafael/.insightface/models/sepaelv2/det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "set det-size: (320, 320)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.ForensicFace at 0x7f4eab4bc3a0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff = ForensicFace(use_gpu=True)\n",
    "ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['keypoints', 'ipd', 'embedding', 'norm', 'aligned_face']),\n",
       " array([[103.600136, 139.8824  ],\n",
       "        [174.26506 , 137.33737 ],\n",
       "        [140.28088 , 187.14763 ],\n",
       "        [109.09409 , 219.34001 ],\n",
       "        [173.40773 , 217.09573 ]], dtype=float32))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = ff.process_image(\"obama.png\")\n",
    "results.keys(), results['keypoints']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparação entre duas imagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "@patch\n",
    "def compare(self:ForensicFace, img1path: str, img2path: str):\n",
    "    img1data = self.process_image(img1path)\n",
    "    assert len(img1data) == 5\n",
    "    img2data = self.process_image(img2path)\n",
    "    assert len(img2data) == 5\n",
    "    return np.dot(img1data[\"embedding\"], img2data[\"embedding\"]) / (\n",
    "        img1data[\"norm\"] * img2data[\"norm\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8556977"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff.compare(\"obama.png\",\"obama2.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agregação de embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "@patch\n",
    "def aggregate_embeddings(self:ForensicFace, embeddings, weights=None):\n",
    "    if weights is None:\n",
    "        weights = np.ones(embeddings.shape[0], dtype=\"int\")\n",
    "    assert embeddings.shape[0] == weights.shape[0]\n",
    "    return np.average(embeddings, axis=0, weights=weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "@patch\n",
    "def aggregate_from_images(self:ForensicFace, list_of_image_paths):\n",
    "    embeddings = []\n",
    "    weights = []\n",
    "    for imgpath in list_of_image_paths:\n",
    "        d = self.process_image(imgpath)\n",
    "        embeddings.append(d[\"embedding\"])\n",
    "    return self.aggregate_embeddings(np.array(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated = ff.aggregate_from_images([\"obama.png\",\"obama2.png\"])\n",
    "aggregated.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "onnxgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "5d2eba9dcae3b57ad0549e14f7337ddce4f7143db9f663634c79a276246cefd7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
